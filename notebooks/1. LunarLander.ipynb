{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32;20m[INFO][common.py@55]: Loaded environment variables: ['HG_TOKEN']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import os\n",
    "\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import (\n",
    "    BaseCallback,\n",
    "    EvalCallback,\n",
    "    CheckpointCallback,\n",
    ")\n",
    "\n",
    "from rl_course.common import show_model, evaluate_model, submit_to_hg_hub, load_dotenv\n",
    "from rich import print as rprint\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v3\"\n",
    "model_name = \"ppo-LunarLander-v6\"\n",
    "model_dir = f\"models/{model_name}\"\n",
    "device = \"cpu\"\n",
    "logs = \"logs\"\n",
    "exp_dir = \"../experiments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedulerCallback(BaseCallback):\n",
    "    def __init__(self, scheduler, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.scheduler = scheduler\n",
    "        self.current_progress = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        self.scheduler.step()\n",
    "        new_lr = self.scheduler.get_last_lr()[0]\n",
    "        self.model.learning_rate = new_lr\n",
    "        self.logger.record(\"train/learning_rate_clbk\", new_lr)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = make_vec_env(env_name, n_envs=16, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    n_steps=2048,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=5,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    device=device,\n",
    "    verbose=0,\n",
    "    tensorboard_log=logs,\n",
    ")\n",
    "scheduler_callback = LRSchedulerCallback(\n",
    "    CyclicLR(\n",
    "        model.policy.optimizer,\n",
    "        base_lr=1e-5,\n",
    "        max_lr=3e-3,\n",
    "        step_size_up=1000,\n",
    "        step_size_down=1_000_000,\n",
    "        mode=\"triangular2\",\n",
    "        cycle_momentum=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "eval_env = make_vec_env(env_name, n_envs=1, vec_env_cls=SubprocVecEnv)\n",
    "best_model_dir = os.path.join(model_dir, \"best_model\")\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=best_model_dir,\n",
    "    log_path=logs,\n",
    "    eval_freq=10000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "ckpt_dir = os.path.join(model_dir, \"checkpoints\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=50000,\n",
    "    save_path=ckpt_dir,\n",
    "    name_prefix=model_name,\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=2_500_000,\n",
    "    callback=[eval_callback, scheduler_callback],\n",
    "    progress_bar=True,\n",
    ")\n",
    "model.save(f\"./models/{model_name}/checkpoints/{model_name}-final\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"models/ppo-LunarLander-v1/checkpoints/ppo-LunarLander-v2-final.zip\"\n",
    "model_path = \"../experiments/ppo-LunarLander-v5/best_model/best_model.zip\"\n",
    "eval_model = PPO.load(model_path, device=\"cpu\")\n",
    "rprint(evaluate_model(env_name, eval_model, num_episodes=10))\n",
    "del eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See the model performance\n",
    "env = gymnasium.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "show_model(env,  PPO.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_to_hg_hub(\n",
    "    PPO.load(model_path, device=\"cpu\"),\n",
    "    \"ppo-LunarLander-v5\",\n",
    "    \"LunarLander-v3\",\n",
    "    \"PPO\",\n",
    "    \"hadhoryth/ppo-LunarLander-v5\",\n",
    "    \"Batman\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
